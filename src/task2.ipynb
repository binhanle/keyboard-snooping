{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from scipy.signal import resample, hilbert, find_peaks\n",
    "from skimage import util\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import Counter\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e907623b1f7249b7a98f3c63a457d339",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=726.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in datasets/atm_pad_9(726,acce)/0320002713.0066: Found 5 instead of 6 peaks\n",
      "Error in datasets/atm_pad_9(726,acce)/0320002713.0087: Found 5 instead of 6 peaks\n",
      "Error in datasets/atm_pad_9(726,acce)/0314003037.0067: Segment too short\n",
      "Error in datasets/atm_pad_9(726,acce)/0320011427.0006: Found 5 instead of 6 peaks\n",
      "Error in datasets/atm_pad_9(726,acce)/0315000919.0084: Found 7 instead of 6 peaks\n",
      "Error in datasets/atm_pad_9(726,acce)/0320002713.0024: Found 5 instead of 6 peaks\n",
      "Error in datasets/atm_pad_9(726,acce)/0320011427.0009: Found 5 instead of 6 peaks\n",
      "Error in datasets/atm_pad_9(726,acce)/0315000919.0001: Segment too short\n",
      "\n",
      "8 of 726 (1.1%) samples discarded\n"
     ]
    }
   ],
   "source": [
    "sampling_rate = 44100\n",
    "frame_bits = 16\n",
    "left_trim_sec = 0.15\n",
    "right_trim_sec = 0.15\n",
    "peak_thresh = 0.15\n",
    "peak_sep_sec = 0.15\n",
    "left_segment_sec = 0.005\n",
    "right_segment_sec = 0.02\n",
    "window_sec = 0.02\n",
    "window_stride = 0.001\n",
    "sensor_samples = 256\n",
    "train_folder = 'datasets/atm_pad_9(726,acce)'\n",
    "\n",
    "def get_audio_signal_from_file(filepath, frame_bits):\n",
    "    df = pd.read_csv(filepath, header=None)\n",
    "    frame_max = 2**(frame_bits - 1)\n",
    "    signal = df.values[:,1] / frame_max\n",
    "    assert np.all((-1 <= signal) & (signal <= 1)), 'Invalid PCM data'\n",
    "#     plt.plot(signal)\n",
    "#     plt.show()\n",
    "    return signal\n",
    "\n",
    "def get_sensor_signal_from_file(filepath):\n",
    "    df = pd.read_csv(filepath, header=None)\n",
    "    signal = df.values[:,1:]\n",
    "#     plt.plot(signal)\n",
    "#     plt.show()\n",
    "    return signal\n",
    "\n",
    "def trim_signal(signal, left_time, right_time, sampling_rate):\n",
    "    start = int(round(left_time * sampling_rate))\n",
    "    end = -int(round(right_time * sampling_rate))\n",
    "#     plt.plot(signal[start:end])\n",
    "#     plt.show()\n",
    "    return signal[start:end]\n",
    "\n",
    "def get_peaks(signal, num_peaks, peak_thresh, peak_sep_sec, sampling_rate):\n",
    "    envelope = np.abs(hilbert(signal))\n",
    "    distance = peak_sep_sec * sampling_rate\n",
    "    peaks, _ = find_peaks(envelope, height=peak_thresh, distance=distance)\n",
    "    assert len(peaks) == num_peaks, \"Found {} instead of {} peaks\".format(len(peaks), num_peaks)\n",
    "#     plt.plot(signal)\n",
    "#     for peak in peaks:\n",
    "#         sample_start = peak - int(round(left_segment_sec * sampling_rate))\n",
    "#         sample_end = peak + int(round(right_segment_sec * sampling_rate))\n",
    "#         plt.axvline(x=peak, color='orange')\n",
    "#         plt.axvline(x=sample_start, color='green')\n",
    "#         plt.axvline(x=sample_end, color='red')\n",
    "        \n",
    "#     plt.show()\n",
    "    return peaks / sampling_rate\n",
    "\n",
    "def get_audio_segments_from_peaks(signal, peaks, left_segment_sec, right_segment_sec, sampling_rate):\n",
    "    segments = []\n",
    "    for peak in peaks:\n",
    "        sample_start = int(round((peak - left_segment_sec) * sampling_rate))\n",
    "        sample_end = int(round((peak + right_segment_sec) * sampling_rate))\n",
    "        segment = signal[sample_start:sample_end + 1]\n",
    "        segments.append(segment)\n",
    "#         plt.plot(signal[sample_start - 100:sample_end + 101])\n",
    "#         plt.axvline(x=100, color='green')\n",
    "#         plt.axvline(x=100 + sample_end - sample_start, color='red')\n",
    "#         plt.show()\n",
    "        \n",
    "    return segments\n",
    "\n",
    "def get_fft_features(segment, window_sec, window_stride, sampling_rate):\n",
    "    N = 2**int(np.ceil(np.log2(window_sec * sampling_rate)))\n",
    "    assert N <= len(segment), \"Segment too short\"\n",
    "    hann = np.hanning(N)\n",
    "    stride = int(round(window_stride * sampling_rate))\n",
    "    windows = util.view_as_windows(segment, window_shape=N, step=stride)\n",
    "    windows = hann * windows\n",
    "    spectrum = np.fft.fft(windows)\n",
    "#     xf = np.fft.fftfreq(N, d=1/sampling_rate)[1:N//2 + 1]\n",
    "#     yf = np.mean(np.abs(spectrum)[:, 1:N//2 + 1], axis=0)\n",
    "#     plt.plot(xf, yf)\n",
    "#     plt.show()\n",
    "    return np.mean(np.abs(spectrum)[:, 1:N//2 + 1], axis=0)\n",
    "\n",
    "def get_data_from_folder(folder, debug_level=0):\n",
    "    data = []\n",
    "    labels = []\n",
    "    sensor_data = []\n",
    "    sensor_targets = []\n",
    "    filepaths = glob.glob(os.path.join(folder, '*'))\n",
    "    total = len(filepaths)\n",
    "    discarded = 0\n",
    "    for filepath in tqdm(filepaths):\n",
    "        try:\n",
    "            audio_file = glob.glob(os.path.join(filepath, '*.wave.csv'))[0]\n",
    "            lnac_files = glob.glob(os.path.join(filepath, '*.*.*.*.acce.csv'))\n",
    "            \n",
    "            chars = Path(audio_file).stem.split('.')[0]\n",
    "        \n",
    "            # Temporary lists\n",
    "            curr_features = []\n",
    "            curr_labels = []\n",
    "            curr_lnac_features = []\n",
    "            curr_lnac_labels = []\n",
    "            \n",
    "            # Process audio\n",
    "            signal = get_audio_signal_from_file(audio_file, frame_bits)\n",
    "            signal = trim_signal(signal, left_trim_sec, right_trim_sec, sampling_rate)\n",
    "            peaks = get_peaks(signal, len(chars), peak_thresh, peak_sep_sec, sampling_rate)\n",
    "            segments = get_audio_segments_from_peaks(signal, peaks, left_segment_sec, right_segment_sec, sampling_rate)\n",
    "            for i, segment in enumerate(segments):\n",
    "                features = get_fft_features(segment, window_sec, window_stride, sampling_rate)\n",
    "                curr_features.append(features)\n",
    "                curr_labels.append(chars[i])\n",
    "                \n",
    "            # Process accelerometer data\n",
    "            for lnac_file in sorted(lnac_files):\n",
    "                signal = get_sensor_signal_from_file(lnac_file)\n",
    "                lnac_features = resample(signal, sensor_samples)\n",
    "                curr_lnac_features.append(lnac_features)\n",
    "                dx, dy = Path(lnac_file).stem.split('.')[2:4]\n",
    "                curr_lnac_labels.append([int(dx), int(dy)])\n",
    "            \n",
    "            # No errors encountered; append data to main lists\n",
    "            data.append(curr_features)\n",
    "            labels.append(curr_labels)\n",
    "            sensor_data.append(curr_lnac_features)\n",
    "            sensor_targets.append(curr_lnac_labels)\n",
    "                    \n",
    "        except Exception as e:\n",
    "            if debug_level > 0:\n",
    "                print('Error in {}: {}'.format(filepath, e))\n",
    "                if debug_level > 1:\n",
    "                    plt.plot(signal)\n",
    "                    plt.show()\n",
    "                \n",
    "            discarded += 1\n",
    "    \n",
    "    print('{} of {} ({:.1f}%) samples discarded'.format(discarded, total, discarded / total * 100))            \n",
    "    return data, labels, sensor_data, sensor_targets\n",
    "\n",
    "print('Loading train data...')\n",
    "audio_data_groups, audio_label_groups, sensor_data_groups, sensor_target_groups = get_data_from_folder(train_folder, debug_level=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "audio_x_train shape: (3012, 512)\n",
      "audio_y_train shape: (3012, 10)\n",
      "sensor_x_train shape: (2510, 256, 3)\n",
      "sensor_y_train shape: (2510, 2)\n",
      "3012 train samples\n",
      "648 val samples\n",
      "648 test samples\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer, StandardScaler\n",
    "\n",
    "val_size = 0.15\n",
    "test_size = 0.15\n",
    "\n",
    "def train_val_test_split(*x, val_size, test_size):\n",
    "    x_train_test = train_test_split(*x, test_size=val_size + test_size)\n",
    "    x_val_test = train_test_split(*x_train_test[1::2], test_size=test_size / (val_size + test_size))\n",
    "    x_train = x_train_test[0::2]\n",
    "    x_val = x_val_test[0::2]\n",
    "    x_test = x_val_test[1::2]\n",
    "    return chain.from_iterable(zip(x_train, x_val, x_test))\n",
    "\n",
    "def flatten_groups(*groups):\n",
    "    return [np.array(list(chain.from_iterable(group))) for group in groups]\n",
    "\n",
    "def sensor_fit(sensor_scaler, sensor_x_in):\n",
    "    _, num_dims = sensor_x_in.shape[1:]\n",
    "    sensor_x = np.reshape(sensor_x_in, (-1, num_dims))\n",
    "    sensor_scaler.fit(sensor_x)\n",
    "    \n",
    "def sensor_transform(sensor_scaler, sensor_x_in):\n",
    "    num_features, num_dims = sensor_x_in.shape[1:]\n",
    "    sensor_x = np.reshape(sensor_x_in, (-1, num_dims))\n",
    "    sensor_x = sensor_scaler.transform(sensor_x)\n",
    "    sensor_x = np.reshape(sensor_x, (-1, num_features, num_dims))\n",
    "    return sensor_x\n",
    "\n",
    "audio_labels_all, = flatten_groups(audio_label_groups)\n",
    "encoder = LabelBinarizer()\n",
    "encoder.fit(audio_labels_all)\n",
    "audio_target_groups = [encoder.transform(group) for group in audio_label_groups]\n",
    "\n",
    "audio_data_train, audio_data_val, audio_data_test, \\\n",
    "audio_labels_train, audio_labels_val, audio_labels_test, \\\n",
    "audio_targets_train, audio_targets_val, audio_targets_test, \\\n",
    "sensor_data_train, sensor_data_val, sensor_data_test, \\\n",
    "sensor_targets_train, sensor_targets_val, sensor_targets_test = train_val_test_split(\n",
    "    audio_data_groups, audio_label_groups, audio_target_groups,\n",
    "    sensor_data_groups, sensor_target_groups,\n",
    "    val_size=val_size, test_size=test_size)\n",
    "\n",
    "audio_x_train, audio_x_val, audio_x_test, audio_y_train, audio_y_val, audio_y_test, \\\n",
    "sensor_x_train, sensor_x_val, sensor_x_test, sensor_y_train, sensor_y_val, sensor_y_test = flatten_groups(\n",
    "    audio_data_train, audio_data_val, audio_data_test,\n",
    "    audio_targets_train, audio_targets_val, audio_targets_test,\n",
    "    sensor_data_train, sensor_data_val, sensor_data_test,\n",
    "    sensor_targets_train, sensor_targets_val, sensor_targets_test)\n",
    "\n",
    "audio_scaler = StandardScaler()\n",
    "audio_scaler.fit(audio_x_train)\n",
    "audio_x_train = audio_scaler.transform(audio_x_train)\n",
    "audio_x_val = audio_scaler.transform(audio_x_val)\n",
    "audio_x_test = audio_scaler.transform(audio_x_test)\n",
    "\n",
    "sensor_scaler = StandardScaler()\n",
    "sensor_fit(sensor_scaler, sensor_x_train)\n",
    "sensor_x_train = sensor_transform(sensor_scaler, sensor_x_train)\n",
    "sensor_x_val = sensor_transform(sensor_scaler, sensor_x_val)\n",
    "sensor_x_test = sensor_transform(sensor_scaler, sensor_x_test)\n",
    "\n",
    "print('audio_x_train shape:', audio_x_train.shape)\n",
    "print('audio_y_train shape:', audio_y_train.shape)\n",
    "print('sensor_x_train shape:', sensor_x_train.shape)\n",
    "print('sensor_y_train shape:', sensor_y_train.shape)\n",
    "print(len(audio_x_train), 'train samples')\n",
    "print(len(audio_x_val), 'val samples')\n",
    "print(len(audio_x_test), 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train label distribution:\n",
      "'0': 426\n",
      "'1': 388\n",
      "'2': 469\n",
      "'3': 446\n",
      "'4': 441\n",
      "'5': 462\n",
      "'6': 423\n",
      "'7': 398\n",
      "'8': 454\n",
      "'9': 401\n"
     ]
    }
   ],
   "source": [
    "def inspect_labels(labels):\n",
    "    dist = Counter(labels)\n",
    "    items = sorted(dist.items())\n",
    "    for label, count in items:\n",
    "        print('{}: {}'.format(repr(label), count))\n",
    "\n",
    "print('Train label distribution:')\n",
    "inspect_labels(audio_labels_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model 1 of 5...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bbe6d7a63e24da6a43b25a49cabc449",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training model 2 of 5...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0939439ecf1e4bd08b871e988b9a87d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training model 3 of 5...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4495f19da9a34a849806de1b2dcdd5af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training model 4 of 5...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8158f1e191df47a7b82c5ae359f066ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training model 5 of 5...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43d3fb2e8c5146c484ef485ccb10c4e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished training\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "from keras.metrics import top_k_categorical_accuracy\n",
    "from keras.utils import to_categorical\n",
    "from keras import backend as K\n",
    "from tqdm.keras import TqdmCallback\n",
    "\n",
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "\n",
    "batch_size = 128\n",
    "epochs = 100\n",
    "patience = 100\n",
    "num_models = 5\n",
    "model_name = 'models/numpad_model'\n",
    "\n",
    "def key_model(num_classes):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, activation='relu', kernel_initializer='he_normal'))\n",
    "    model.add(Dense(64, activation='relu', kernel_initializer='he_normal'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='softmax', kernel_initializer='he_normal'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "def rounded_acc(y_true, y_pred):\n",
    "    return K.mean(K.equal(K.round(y_true), K.round(y_pred)))\n",
    "\n",
    "def train_model(model, x_train, y_train, x_val, y_val, filename):\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=patience)\n",
    "    mc = ModelCheckpoint(filename, monitor='val_loss', mode='min', verbose=0, save_best_only=True)\n",
    "    tc = TqdmCallback(verbose=0)\n",
    "\n",
    "    model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              verbose=0,\n",
    "              validation_data=(x_val, y_val),\n",
    "              callbacks=[es, mc, tc])\n",
    "\n",
    "    return load_model(filename)\n",
    "\n",
    "def train_ensemble(model_spec, x_train, y_train, x_val, y_val):\n",
    "    models = []\n",
    "    \n",
    "    for i in range(num_models):\n",
    "        filename = '{}_{}.h5'.format(model_name, i)\n",
    "        print('Training model {} of {}...'.format(i + 1, num_models))\n",
    "        num_classes = len(y_train[0])\n",
    "        model = model_spec(num_classes)\n",
    "        model = train_model(model, x_train, y_train, x_val, y_val, filename)\n",
    "        models.append(model)\n",
    "\n",
    "    print('Finished training')\n",
    "    return models\n",
    "    \n",
    "models = train_ensemble(key_model, audio_x_train, audio_y_train, audio_x_val, audio_y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val accuracy: 84.0%\n",
      "Top-3 accuracy: 96.3%\n"
     ]
    }
   ],
   "source": [
    "def top_k_accuracy_score(y_pred, y_test, k):\n",
    "    top_k_marks = top_k_categorical_accuracy(y_test, y_pred, k=k)\n",
    "    return sum(top_k_marks) / len(top_k_marks)\n",
    "\n",
    "def rounded_acc_score(y_pred, y_test):\n",
    "    return np.mean(np.equal(np.round(y_pred), np.round(y_test)))\n",
    "\n",
    "def predict_ensemble(models, x):\n",
    "    sum_pred = 0\n",
    "    for model in models:\n",
    "        sum_pred += model.predict(x)\n",
    "        \n",
    "    return sum_pred / len(models)\n",
    "\n",
    "def evaluate_ensemble(models, x, y, k):\n",
    "    avg_pred = predict_ensemble(models, x)\n",
    "    labels_pred = np.argmax(avg_pred, axis=1)\n",
    "    y_pred = to_categorical(labels_pred, y.shape[1])\n",
    "    score = accuracy_score(y_pred, y)\n",
    "    top_k_score = top_k_accuracy_score(avg_pred, y, k)\n",
    "    return score, top_k_score\n",
    "\n",
    "def evaluate_ensemble_reg(models, x, y):\n",
    "    avg_pred = predict_ensemble(models, x)\n",
    "    score = rounded_acc_score(avg_pred, y)\n",
    "    loss = np.mean((avg_pred - y)**2)\n",
    "    return score, loss\n",
    "\n",
    "k = 3\n",
    "score, top_k_score = evaluate_ensemble(models, audio_x_val, audio_y_val, k)\n",
    "\n",
    "print('Val accuracy: {:.1f}%'.format(score * 100))\n",
    "print('Top-{} accuracy: {:.1f}%'.format(k, top_k_score * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1 val accuracy: 80.9%\n",
      "Model 2 val accuracy: 79.9%\n",
      "Model 3 val accuracy: 81.3%\n",
      "Model 4 val accuracy: 77.9%\n",
      "Model 5 val accuracy: 79.9%\n"
     ]
    }
   ],
   "source": [
    "for i, model in enumerate(models):\n",
    "    score = model.evaluate(audio_x_val, audio_y_val, verbose=0)\n",
    "    print('Model {} val accuracy: {:.1f}%'.format(i + 1, score[1] * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 88.1%\n",
      "Top-3 accuracy: 96.9%\n"
     ]
    }
   ],
   "source": [
    "score, top_k_score = evaluate_ensemble(models, audio_x_test, audio_y_test, k)\n",
    "\n",
    "print('Test accuracy: {:.1f}%'.format(score * 100))\n",
    "print('Top-{} accuracy: {:.1f}%'.format(k, top_k_score * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1 test accuracy: 83.0%\n",
      "Model 2 test accuracy: 83.3%\n",
      "Model 3 test accuracy: 84.7%\n",
      "Model 4 test accuracy: 83.5%\n",
      "Model 5 test accuracy: 85.3%\n"
     ]
    }
   ],
   "source": [
    "for i, model in enumerate(models):\n",
    "    score = model.evaluate(audio_x_test, audio_y_test, verbose=0)\n",
    "    print('Model {} test accuracy: {:.1f}%'.format(i + 1, score[1] * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model 1 of 5...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05f0c896d41c409494dbd3144e0c6af1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training model 2 of 5...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e420082d3e39433cb07365908cd0a58a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training model 3 of 5...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "621eb42f12104aafbd812317b530f7ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training model 4 of 5...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2df11130bd5f4762a9320c99cc79dd2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training model 5 of 5...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b193022a753248749978d0ded4c2bd7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished training\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Flatten\n",
    "\n",
    "def dir_model(num_classes):\n",
    "    model = Sequential()\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(64, activation='relu', kernel_initializer='he_normal'))\n",
    "    model.add(Dense(64, activation='relu', kernel_initializer='he_normal'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes))\n",
    "\n",
    "    keras.metrics.rounded_acc = rounded_acc\n",
    "    model.compile(loss='mean_squared_error',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['rounded_acc'])\n",
    "\n",
    "    return model\n",
    "\n",
    "epochs = 100\n",
    "patience = 100\n",
    "sensor_models = train_ensemble(dir_model, sensor_x_train, sensor_y_train, sensor_x_val, sensor_y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val accuracy: 80.3%\n",
      "Val loss: 0.166\n",
      "\n",
      "Model 1 val accuracy: 77.8%\n",
      "Model 2 val accuracy: 77.8%\n",
      "Model 3 val accuracy: 78.6%\n",
      "Model 4 val accuracy: 76.8%\n",
      "Model 5 val accuracy: 78.2%\n"
     ]
    }
   ],
   "source": [
    "score, loss = evaluate_ensemble_reg(sensor_models, sensor_x_val, sensor_y_val)\n",
    "print('Val accuracy: {:.1f}%'.format(score * 100))\n",
    "print('Val loss: {:.3g}\\n'.format(loss))\n",
    "\n",
    "for i, model in enumerate(sensor_models):\n",
    "    score = model.evaluate(sensor_x_val, sensor_y_val, verbose=0)\n",
    "    print('Model {} val accuracy: {:.1f}%'.format(i + 1, score[1] * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.7682274   0.7900537 ]\n",
      " [-0.842597    0.67386925]\n",
      " [ 0.01621329 -1.5949844 ]\n",
      " [ 1.7678831   0.29708225]\n",
      " [-0.426646    0.60147953]]\n",
      "[[ 1  1]\n",
      " [-1  1]\n",
      " [ 0 -2]\n",
      " [ 2  0]\n",
      " [ 0  0]]\n"
     ]
    }
   ],
   "source": [
    "print(predict_ensemble(sensor_models, sensor_x_test[:5]))\n",
    "print(sensor_y_test[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 80.8%\n",
      "Test loss: 0.155\n",
      "\n",
      "Model 1 test accuracy: 78.6%\n",
      "Model 2 test accuracy: 78.6%\n",
      "Model 3 test accuracy: 78.8%\n",
      "Model 4 test accuracy: 77.5%\n",
      "Model 5 test accuracy: 77.4%\n"
     ]
    }
   ],
   "source": [
    "score, loss = evaluate_ensemble_reg(sensor_models, sensor_x_test, sensor_y_test)\n",
    "print('Test accuracy: {:.1f}%'.format(score * 100))\n",
    "print('Test loss: {:.3g}\\n'.format(loss))\n",
    "\n",
    "for i, model in enumerate(sensor_models):\n",
    "    score = model.evaluate(sensor_x_test, sensor_y_test, verbose=0)\n",
    "    print('Model {} test accuracy: {:.1f}%'.format(i + 1, score[1] * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (audio only):  88.1%\n",
      "Accuracy (audio+acce):  91.5%\n"
     ]
    }
   ],
   "source": [
    "def key_disp(x, y):\n",
    "    h_dist = (y - 1) % 3 - (x - 1) % 3\n",
    "    v_dist = (y - 1) // 3 - (x - 1) // 3\n",
    "    if x == 0:\n",
    "        h_dist += 1\n",
    "        \n",
    "    if y == 0:\n",
    "        h_dist -= 1\n",
    "        \n",
    "    return np.array([h_dist, v_dist])\n",
    "\n",
    "def gaussian_rbf(disp1, disp2):\n",
    "    return np.exp(-c * np.sum((disp1 - disp2)**2))\n",
    "\n",
    "def evaluate_ensemble_fused_old(models, audio_data_test, audio_labels_test, sensor_data_test):\n",
    "    correct = 0\n",
    "    correct2 = 0\n",
    "\n",
    "    for audio_data_group, audio_label_group, sensor_data_group in zip(\n",
    "        audio_data_test, audio_labels_test, sensor_data_test):\n",
    "        x = np.array(audio_data_group)\n",
    "        x = audio_scaler.transform(x)\n",
    "        y_pred = predict_ensemble(models, x)\n",
    "        labels_pred = encoder.inverse_transform(y_pred)\n",
    "        string_pred = ''.join(labels_pred)\n",
    "        string_true = ''.join(audio_label_group)\n",
    "\n",
    "        curr_key = np.argmax(y_pred[0])\n",
    "        sensor_x = np.array(sensor_data_group)\n",
    "        sensor_x = sensor_transform(sensor_scaler, sensor_x)\n",
    "        sensor_y_pred = predict_ensemble(sensor_models, sensor_x)\n",
    "        for i in range(1, len(audio_label_group)):\n",
    "            disp_probs = np.array([gaussian_rbf(key_disp(curr_key, k), sensor_y_pred[i - 1]) for k in range(10)])\n",
    "            y_pred[i] *= disp_probs\n",
    "            curr_key = np.argmax(y_pred[i])\n",
    "\n",
    "        labels_pred = encoder.inverse_transform(y_pred)\n",
    "        string_pred2 = ''.join(labels_pred)\n",
    "\n",
    "        correct += sum(c1 == c2 for c1, c2 in zip(string_pred, string_true))\n",
    "        correct2 += sum(c1 == c2 for c1, c2 in zip(string_pred2, string_true))\n",
    "\n",
    "    audio_acc = correct / len(audio_data_test) / len(audio_data_test[0])\n",
    "    fused_acc = correct2 / len(audio_data_test) / len(audio_data_test[0])\n",
    "    return audio_acc, fused_acc\n",
    "\n",
    "c = 1\n",
    "audio_acc_old, fused_acc_old = evaluate_ensemble_fused_old(models, audio_data_test, audio_labels_test, sensor_data_test)\n",
    "print('Accuracy (audio only):  {:.1f}%'.format(audio_acc_old * 100))\n",
    "print('Accuracy (audio+acce):  {:.1f}%'.format(fused_acc_old * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ag_x_train = np.array([audio_scaler.transform(group) for group in audio_data_train])\n",
    "sg_x_train = np.array([sensor_transform(sensor_scaler, np.array(group)) for group in sensor_data_train])\n",
    "ag_y_train = np.array(audio_targets_train)\n",
    "sg_y_train = np.array(sensor_targets_train)\n",
    "ag_x_val = np.array([audio_scaler.transform(group) for group in audio_data_val])\n",
    "sg_x_val = np.array([sensor_transform(sensor_scaler, np.array(group)) for group in sensor_data_val])\n",
    "ag_y_val = np.array(audio_targets_val)\n",
    "sg_y_val = np.array(sensor_targets_val)\n",
    "ag_x_test = np.array([audio_scaler.transform(group) for group in audio_data_test])\n",
    "sg_x_test = np.array([sensor_transform(sensor_scaler, np.array(group)) for group in sensor_data_test])\n",
    "ag_y_test = np.array(audio_targets_test)\n",
    "sg_y_test = np.array(sensor_targets_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def key_pos(k):\n",
    "    key_locs = np.array([\n",
    "        [0, 0],\n",
    "        [-1, 1], [0, 1], [1, 1],\n",
    "        [-1, 2], [0, 2], [1, 2],\n",
    "        [-1, 3], [0, 3], [1, 3]\n",
    "    ])\n",
    "    return key_locs[k]\n",
    "\n",
    "def log_ll(keys_pred, dirs_pred, guess):\n",
    "    keys_pred /= np.vstack(np.sum(keys_pred, axis=1))\n",
    "    key_probs = keys_pred[range(len(guess)), guess]\n",
    "    log_key_prob = np.sum(np.log(key_probs))\n",
    "    \n",
    "    key_locs = key_pos(guess)\n",
    "    disps = np.diff(key_locs, axis=0)\n",
    "    log_dir_prob = -c * np.sum((disps - dirs_pred[:len(guess) - 1])**2)\n",
    "    return log_key_prob + log_dir_prob\n",
    "\n",
    "def max_log_ll(keys_pred, dirs_pred):\n",
    "    best_guess = np.argmax(keys_pred, axis=1)\n",
    "    max_log_ll = log_ll(keys_pred, dirs_pred, best_guess)\n",
    "    \n",
    "    def search_starting_from(guess):\n",
    "        nonlocal best_guess, max_log_ll\n",
    "        if len(guess) > 0:\n",
    "            curr_log_ll = log_ll(keys_pred, dirs_pred, guess)\n",
    "            if curr_log_ll <= max_log_ll:\n",
    "                return\n",
    "            \n",
    "            if len(guess) == len(keys_pred):\n",
    "                if curr_log_ll > max_log_ll:\n",
    "                    max_log_ll = curr_log_ll\n",
    "                    best_guess = guess\n",
    "                    \n",
    "                return\n",
    "        \n",
    "        new_guesses = [np.append(guess, x).astype(int) for x in range(keys_pred.shape[1])]\n",
    "        for new_guess in new_guesses:\n",
    "            search_starting_from(new_guess)\n",
    "        \n",
    "    search_starting_from([])\n",
    "    return best_guess\n",
    "\n",
    "def evaluate_ensemble_fused(audio_models, sensor_models, audio_data, sensor_data, audio_targets):\n",
    "    audio_correct = 0\n",
    "    fused_correct = 0\n",
    "    for audio_x, sensor_x, audio_y in zip(audio_data, sensor_data, audio_targets):\n",
    "        audio_pred = predict_ensemble(audio_models, audio_x)\n",
    "        sensor_pred = predict_ensemble(sensor_models, sensor_x)\n",
    "        string_pred_audio = np.argmax(audio_pred, axis=-1)\n",
    "        string_pred_fused = max_log_ll(audio_pred, sensor_pred)\n",
    "        string_true = np.argmax(audio_y, axis=-1)\n",
    "        \n",
    "        audio_correct += sum(string_pred_audio == string_true)\n",
    "        fused_correct += sum(string_pred_fused == string_true)\n",
    "\n",
    "    audio_acc = audio_correct / len(audio_data) / len(audio_data[0])\n",
    "    fused_acc = fused_correct / len(audio_data) / len(audio_data[0])\n",
    "    return audio_acc, fused_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (audio only, individual key):         88.1%\n",
      "Accuracy (audio+acce, MLTS, individual key):   93.7%\n",
      "Accuracy (audio+acce, MLTS, full PIN, top 5):  90.7%\n"
     ]
    }
   ],
   "source": [
    "def max_log_ll_top_k(keys_pred, dirs_pred, k):\n",
    "    best_guess = np.argmax(keys_pred, axis=1)\n",
    "    max_log_ll = log_ll(keys_pred, dirs_pred, best_guess)\n",
    "    best_guesses = [(max_log_ll, 0, best_guess)]\n",
    "    iterations = 0\n",
    "    \n",
    "    def search_starting_from(guess):\n",
    "        nonlocal best_guesses, iterations\n",
    "        iterations += 1\n",
    "        if len(guess) > 0:\n",
    "            curr_log_ll = log_ll(keys_pred, dirs_pred, guess)\n",
    "            if len(best_guesses) == k and curr_log_ll <= best_guesses[0][0]:\n",
    "                return\n",
    "            \n",
    "            if len(guess) == len(keys_pred):\n",
    "                if (len(best_guesses) < k or curr_log_ll > best_guesses[0][0]) and not np.array_equal(guess, best_guess):\n",
    "                    best_guesses.append((curr_log_ll, iterations, guess))\n",
    "                    best_guesses = sorted(best_guesses)[-k:]\n",
    "                    \n",
    "                return\n",
    "        \n",
    "        new_guesses = [np.append(guess, x).astype(int) for x in range(keys_pred.shape[1])]\n",
    "        for new_guess in new_guesses:\n",
    "            search_starting_from(new_guess)\n",
    "        \n",
    "    search_starting_from([])\n",
    "    return best_guesses\n",
    "\n",
    "def evaluate_ensemble_fused_top_k(audio_models, sensor_models, audio_data, sensor_data, audio_targets, k):\n",
    "    fused_correct = 0\n",
    "    for audio_x, sensor_x, audio_y in zip(audio_data, sensor_data, audio_targets):\n",
    "        audio_pred = predict_ensemble(audio_models, audio_x)\n",
    "        sensor_pred = predict_ensemble(sensor_models, sensor_x)\n",
    "        string_preds_fused = max_log_ll_top_k(audio_pred, sensor_pred, k)\n",
    "        string_true = np.argmax(audio_y, axis=-1)\n",
    "        \n",
    "        fused_correct += sum(np.all(pred[2] == string_true) for pred in string_preds_fused)\n",
    "\n",
    "    fused_acc = fused_correct / len(audio_data)\n",
    "    return fused_acc\n",
    "\n",
    "k = 5\n",
    "audio_acc, fused_acc = evaluate_ensemble_fused(models, sensor_models, ag_x_test, sg_x_test, ag_y_test)\n",
    "fused_acc_top_k = evaluate_ensemble_fused_top_k(models, sensor_models, ag_x_test, sg_x_test, ag_y_test, k)\n",
    "print('Accuracy (audio only, individual key):         {:.1f}%'.format(audio_acc * 100))\n",
    "print('Accuracy (audio+acce, MLTS, individual key):   {:.1f}%'.format(fused_acc * 100))\n",
    "print('Accuracy (audio+acce, MLTS, full PIN, top {}):  {:.1f}%'.format(k, fused_acc_top_k * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "046a4177124340329c149d29d0e45cc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=11.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0 of 11 (0.0%) samples discarded\n",
      "ag_x_test shape: (11, 6, 512)\n",
      "ag_y_test shape: (11, 6, 10)\n",
      "sg_x_test shape: (11, 5, 256, 3)\n",
      "sg_y_test shape: (11, 5, 2)\n"
     ]
    }
   ],
   "source": [
    "test_folder = 'datasets/atm_pad_7(11,acce)'\n",
    "peak_thresh = 0.8\n",
    "peak_sep_sec = 0.2\n",
    "\n",
    "print('Loading test data...')\n",
    "audio_data_groups, audio_label_groups, sensor_data_groups, sensor_target_groups = get_data_from_folder(test_folder, debug_level=1)\n",
    "\n",
    "audio_target_groups = [encoder.transform(group) for group in audio_label_groups]\n",
    "ag_x_test = np.array([audio_scaler.transform(group) for group in audio_data_groups])\n",
    "sg_x_test = np.array([sensor_transform(sensor_scaler, np.array(group)) for group in sensor_data_groups])\n",
    "ag_y_test = np.array(audio_target_groups)\n",
    "sg_y_test = np.array(sensor_target_groups)\n",
    "\n",
    "print('ag_x_test shape:', ag_x_test.shape)\n",
    "print('ag_y_test shape:', ag_y_test.shape)\n",
    "print('sg_x_test shape:', sg_x_test.shape)\n",
    "print('sg_y_test shape:', sg_y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 54.5%\n",
      "Test loss: 0.427\n",
      "\n",
      "Model 1 test accuracy: 56.6%\n",
      "Model 2 test accuracy: 43.8%\n",
      "Model 3 test accuracy: 56.6%\n",
      "Model 4 test accuracy: 41.8%\n",
      "Model 5 test accuracy: 47.5%\n"
     ]
    }
   ],
   "source": [
    "sensor_x_test, sensor_y_test = flatten_groups(sg_x_test, sg_y_test)\n",
    "\n",
    "score, loss = evaluate_ensemble_reg(sensor_models, sensor_x_test, sensor_y_test)\n",
    "print('Test accuracy: {:.1f}%'.format(score * 100))\n",
    "print('Test loss: {:.3g}\\n'.format(loss))\n",
    "\n",
    "for i, model in enumerate(sensor_models):\n",
    "    score = model.evaluate(sensor_x_test, sensor_y_test, verbose=0)\n",
    "    print('Model {} test accuracy: {:.1f}%'.format(i + 1, score[1] * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anle/anaconda3/envs/keyboard-ml/lib/python3.7/site-packages/ipykernel_launcher.py:13: RuntimeWarning: divide by zero encountered in log\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (audio only):              12.1%\n",
      "Accuracy (audio+acce, MLTS, c=1):   19.7%\n",
      "Accuracy (audio+acce, MLTS, c=100): 27.3%\n"
     ]
    }
   ],
   "source": [
    "c = 1\n",
    "audio_acc, fused_acc_1 = evaluate_ensemble_fused(models, sensor_models, ag_x_test, sg_x_test, ag_y_test)\n",
    "\n",
    "c = 100\n",
    "_, fused_acc_100 = evaluate_ensemble_fused(models, sensor_models, ag_x_test, sg_x_test, ag_y_test)\n",
    "print('Accuracy (audio only):              {:.1f}%'.format(audio_acc * 100))\n",
    "print('Accuracy (audio+acce, MLTS, c=1):   {:.1f}%'.format(fused_acc_1 * 100))\n",
    "print('Accuracy (audio+acce, MLTS, c=100): {:.1f}%'.format(fused_acc_100 * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
