{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"colab":{"name":"task1.ipynb","provenance":[]}},"cells":[{"cell_type":"code","metadata":{"scrolled":true,"id":"u3IRfU4NuKxd","colab_type":"code","colab":{}},"source":["import os\n","import glob\n","import numpy as np\n","import pandas as pd\n","import soundfile as sf\n","import matplotlib.pyplot as plt\n","from pathlib import Path\n","from scipy.signal import resample, hilbert, find_peaks\n","from skimage import util\n","from tqdm.notebook import tqdm\n","from collections import Counter"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q3HbPEEOuKxl","colab_type":"code","colab":{"referenced_widgets":["506b902eaa1a4dbc98386a7d13012450"]},"outputId":"2ff74b5f-33c6-456e-90a9-5f081a896d02"},"source":["sampling_rate = 44100\n","frame_bits = 16\n","left_trim_sec = 0.1\n","right_trim_sec = 0.1\n","peak_thresh = 0.5\n","peak_sep_sec = 0.2\n","left_segment_sec = 0.005\n","right_segment_sec = 0.02\n","window_sec = 0.02\n","window_stride = 0.001\n","train_folder = 'all_data'\n","\n","def get_signal_from_file(filepath, ref_samp_rate, frame_bits):\n","    df = pd.read_csv(filepath, header=None)\n","    frame_max = 2**(frame_bits - 1)\n","    signal = df.values[:,1] / frame_max\n","    assert np.all((-1 <= signal) & (signal <= 1)), 'Invalid PCM data'\n","#     plt.plot(signal)\n","#     plt.show()\n","    return signal\n","\n","def trim_signal(signal, left_time, right_time, sampling_rate):\n","    start = int(round(left_time * sampling_rate))\n","    end = -int(round(right_time * sampling_rate))\n","#     plt.plot(signal[start:end])\n","#     plt.show()\n","    return signal[start:end]\n","\n","def get_peaks(signal, num_peaks, peak_thresh, peak_sep_sec, sampling_rate):\n","    envelope = np.abs(hilbert(signal))\n","    distance = peak_sep_sec * sampling_rate\n","    peaks, _ = find_peaks(envelope, height=peak_thresh, distance=distance)\n","    assert len(peaks) == num_peaks, \"Found {} instead of {} peaks\".format(len(peaks), num_peaks)\n","    return peaks\n","\n","def get_segments_from_peaks(signal, peaks, left_segment_sec, right_segment_sec, sampling_rate):\n","    segments = []\n","    for peak in peaks:\n","        sample_start = peak - int(round(left_segment_sec * sampling_rate))\n","        sample_end = peak + int(round(right_segment_sec * sampling_rate))\n","        segment = signal[sample_start:sample_end + 1]\n","        segments.append(segment)\n","#         plt.plot(signal[sample_start - 100:sample_end + 101])\n","#         plt.axvline(x=100, color='green')\n","#         plt.axvline(x=100 + sample_end - sample_start, color='red')\n","#         plt.show()\n","        \n","    return segments\n","\n","def get_fft_features(segment, window_sec, window_stride, sampling_rate):\n","    N = 2**int(np.ceil(np.log2(window_sec * sampling_rate)))\n","    assert N <= len(segment), \"Segment too short\"\n","    hann = np.hanning(N)\n","    stride = int(round(window_stride * sampling_rate))\n","    windows = util.view_as_windows(segment, window_shape=N, step=stride)\n","    windows = hann * windows\n","    spectrum = np.fft.fft(windows)\n","#     xf = np.fft.fftfreq(N, d=1/sampling_rate)[1:N//2 + 1]\n","#     yf = np.mean(np.abs(spectrum)[:, 1:(N - 1)//2 + 1], axis=0)\n","#     plt.plot(xf, yf)\n","    return np.mean(np.abs(spectrum)[:, 1:(N - 1)//2 + 1], axis=0)\n","\n","def get_data_from_folder(folder, debug=False):\n","    data = []\n","    labels = []\n","    filepaths = glob.glob(os.path.join(folder, '*', '*.wave.csv'))\n","    total = len(filepaths)\n","    discarded = 0\n","    for filepath in tqdm(filepaths):\n","        try:\n","            chars = Path(filepath).stem.split('.')[0]\n","            signal = get_signal_from_file(filepath, sampling_rate, frame_bits)\n","            signal = trim_signal(signal, left_trim_sec, right_trim_sec, sampling_rate)\n","            peaks = get_peaks(signal, len(chars), peak_thresh, peak_sep_sec, sampling_rate)\n","            segments = get_segments_from_peaks(signal, peaks, left_segment_sec, right_segment_sec, sampling_rate)\n","            for i, segment in enumerate(segments):\n","                features = get_fft_features(segment, window_sec, window_stride, sampling_rate)\n","                data.append(features)\n","                labels.append(chars[i])\n","        except AssertionError as e:\n","            if debug:\n","                print('Error in {}: {}'.format(filepath, e))\n","                \n","            discarded += 1\n","    \n","    print('{} of {} ({:.1f}%) samples discarded'.format(discarded, total, discarded / total * 100))            \n","    data = np.array(data)\n","    labels = np.array(labels)\n","    return data, labels\n","\n","print('Loading train data...')\n","data_train_all, labels_train_all = get_data_from_folder(train_folder)\n","print('Train data shape:', data_train_all.shape)\n","print('Train labels shape:', labels_train_all.shape)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Loading train data...\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"506b902eaa1a4dbc98386a7d13012450","version_major":2,"version_minor":0},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=5152.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","343 of 5152 (6.7%) samples discarded\n","Train data shape: (4809, 511)\n","Train labels shape: (4809,)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"8nCS-zeLuKxr","colab_type":"code","colab":{},"outputId":"143e4cf8-c822-43ad-e1c3-83fb13829b9b"},"source":["def inspect_labels(labels):\n","    dist = Counter(labels)\n","    items = sorted(dist.items())\n","    for label, count in items:\n","        print('{}: {}'.format(repr(label), count))\n","        \n","print('Train label distribution:')\n","inspect_labels(labels_train_all)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Train label distribution:\n","'0': 122\n","'1': 123\n","'2': 124\n","'3': 132\n","'4': 120\n","'5': 142\n","'6': 125\n","'7': 135\n","'8': 141\n","'9': 128\n","'a': 133\n","'b': 122\n","'c': 125\n","'d': 143\n","'e': 133\n","'f': 137\n","'g': 121\n","'h': 135\n","'i': 136\n","'j': 143\n","'k': 122\n","'l': 132\n","'m': 147\n","'n': 125\n","'o': 171\n","'p': 146\n","'q': 129\n","'r': 127\n","'s': 125\n","'t': 130\n","'u': 141\n","'v': 138\n","'w': 145\n","'x': 134\n","'y': 137\n","'z': 140\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"scrolled":true,"id":"71E6zp9QuKxw","colab_type":"code","colab":{"referenced_widgets":["7a9a389feb064a0c872f32cdd7680cf7","99ffbe0d324c44fb9c339319b3c1aee6","c337b2036ef5458ba81bfdae5474b566","13cada047d2442ffb854b0328e21e107","fe38a28ae76f42c0b3a22bd65814c632"]},"outputId":"893773f7-d448-4994-e15c-89fa0a085766"},"source":["import keras\n","from sklearn.preprocessing import LabelBinarizer, StandardScaler\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","from keras.models import Sequential\n","from keras.layers import Dense, Dropout\n","from keras.callbacks import EarlyStopping, ModelCheckpoint\n","from keras.models import load_model\n","from keras.metrics import top_k_categorical_accuracy\n","from keras.utils import to_categorical\n","from keras import backend as K\n","from tqdm.keras import TqdmCallback\n","\n","from keras.backend.tensorflow_backend import set_session\n","import tensorflow as tf\n","gpus = tf.config.experimental.list_physical_devices('GPU')\n","tf.config.experimental.set_memory_growth(gpus[0], True)\n","\n","batch_size = 128\n","epochs = 100\n","patience = 100\n","val_size = 0.15\n","test_size = 0.15\n","num_models = 5\n","model_name = 'key_model'\n","\n","def key_model(num_classes):\n","    model = Sequential()\n","    model.add(Dense(64, activation='relu', kernel_initializer='he_normal'))\n","    model.add(Dense(64, activation='relu', kernel_initializer='he_normal'))\n","    model.add(Dropout(0.5))\n","    model.add(Dense(num_classes, activation='softmax'))\n","\n","    model.compile(loss=keras.losses.categorical_crossentropy,\n","                  optimizer=keras.optimizers.Adam(),\n","                  metrics=['accuracy'])\n","\n","    return model\n","\n","def train_model(model, x_train, y_train, x_val, y_val, filename):\n","    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=patience)\n","    mc = ModelCheckpoint(filename, monitor='val_accuracy', mode='max', verbose=0, save_best_only=True)\n","    tc = TqdmCallback(verbose=0)\n","\n","    model.fit(x_train, y_train,\n","              batch_size=batch_size,\n","              epochs=epochs,\n","              verbose=0,\n","              validation_data=(x_val, y_val),\n","              callbacks=[es, mc, tc])\n","\n","    return load_model(filename)\n","\n","def preprocess_data_test(data_train, labels_train, data_test, labels_test):\n","    encoder = LabelBinarizer()\n","    onehot = encoder.fit_transform(labels_train_all)\n","    num_classes = onehot.shape[1]\n","    x_train, x_val, y_train, y_val, labels_train, labels_val = train_test_split(data_train_all, onehot, labels_train_all, test_size=val_size, stratify=labels_train_all)\n","    \n","    input_shape = data_train_all[0].shape\n","    if debug:\n","        print(input_shape)\n","\n","    scaler = StandardScaler()\n","    scaler.fit(x_train)\n","    x_train = scaler.transform(x_train)\n","    x_val = scaler.transform(x_val)\n","    x_test = scaler.transform(data_test)\n","    y_test = encoder.transform(labels_test)\n","    if debug:\n","        print('x_train shape:', x_train.shape)\n","        print('y_train shape:', y_train.shape)\n","        print(x_train.shape[0], 'train samples')\n","        print(x_val.shape[0], 'val samples')\n","        print(x_test.shape[0], 'test samples')\n","\n","def preprocess_data(data_train_all, labels_train_all):\n","    encoder = LabelBinarizer()\n","    onehot = encoder.fit_transform(labels_train_all)\n","    num_classes = onehot.shape[1]\n","    x_train, x_test, y_train, y_test, labels_train, labels_test = train_test_split(data_train_all, onehot, labels_train_all, test_size=val_size + test_size, stratify=labels_train_all)\n","    x_val, x_test, y_val, y_test, labels_val, labels_test = train_test_split(x_test, y_test, labels_test, test_size=test_size / (val_size + test_size), stratify=labels_test)\n","\n","    input_shape = data_train_all[0].shape\n","\n","    scaler = StandardScaler()\n","    scaler.fit(x_train)\n","    x_train = scaler.transform(x_train)\n","    x_val = scaler.transform(x_val)\n","    x_test = scaler.transform(x_test)\n","    y_test = encoder.transform(labels_test)\n","    print('x_train shape:', x_train.shape)\n","    print('y_train shape:', y_train.shape)\n","    print(x_train.shape[0], 'train samples')\n","    print(x_val.shape[0], 'val samples')\n","    print(x_test.shape[0], 'test samples')\n","    return x_train, y_train, x_val, y_val, x_test, y_test\n","    \n","x_train, y_train, x_val, y_val, x_test, y_test = preprocess_data(data_train_all, labels_train_all)\n","\n","models = []\n","for i in range(num_models):\n","    filename = '{}_{}.h5'.format(model_name, i)\n","    print('Training model {} of {}...'.format(i + 1, num_models))\n","    num_classes = y_train.shape[1]\n","    model = key_model(num_classes)\n","    model = train_model(model, x_train, y_train, x_val, y_val, filename)\n","    models.append(model)\n","    \n","print('Finished training')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"},{"output_type":"stream","text":["x_train shape: (3366, 511)\n","y_train shape: (3366, 36)\n","3366 train samples\n","721 val samples\n","722 test samples\n","Training model 1 of 5...\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7a9a389feb064a0c872f32cdd7680cf7","version_major":2,"version_minor":0},"text/plain":["HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","Training model 2 of 5...\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"99ffbe0d324c44fb9c339319b3c1aee6","version_major":2,"version_minor":0},"text/plain":["HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","Training model 3 of 5...\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c337b2036ef5458ba81bfdae5474b566","version_major":2,"version_minor":0},"text/plain":["HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","Training model 4 of 5...\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"13cada047d2442ffb854b0328e21e107","version_major":2,"version_minor":0},"text/plain":["HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","Training model 5 of 5...\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fe38a28ae76f42c0b3a22bd65814c632","version_major":2,"version_minor":0},"text/plain":["HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","Finished training\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"5VKZhc9auKx0","colab_type":"code","colab":{},"outputId":"4bb9494d-6514-4eb5-8093-48458fcbdc19"},"source":["def top_k_accuracy_score(y_pred, y_test, k):\n","    top_k_marks = top_k_categorical_accuracy(y_test, y_pred, k=k)\n","    return sum(top_k_marks) / len(top_k_marks)\n","\n","k = 5\n","avg_pred = sum(model.predict(x_val) for model in models)\n","labels_pred = np.argmax(avg_pred, axis=1)\n","y_pred = to_categorical(labels_pred)\n","score = accuracy_score(y_pred, y_val)\n","top_k_score = top_k_accuracy_score(avg_pred, y_val, k)\n","print('Val accuracy: {:.1f}%'.format(score * 100))\n","print('Top-{} accuracy: {:.1f}%'.format(k, top_k_score * 100))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Val accuracy: 99.0%\n","Top-5 accuracy: 99.9%\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"xG6AO6-EuKx5","colab_type":"code","colab":{},"outputId":"10edab5d-4de0-4d73-bba4-b1a2e018ad4d"},"source":["for i, model in enumerate(models):\n","    score = model.evaluate(x_val, y_val, verbose=0)\n","    print('Model {} val accuracy: {:.1f}%'.format(i + 1, score[1] * 100))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Model 1 val accuracy: 97.8%\n","Model 2 val accuracy: 97.4%\n","Model 3 val accuracy: 98.1%\n","Model 4 val accuracy: 97.5%\n","Model 5 val accuracy: 98.5%\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"mWQRhndNuKx9","colab_type":"code","colab":{},"outputId":"e7dbb63a-73f7-4bdd-9ada-2d20c7c68f04"},"source":["avg_pred = sum(model.predict(x_test) for model in models)\n","labels_pred = np.argmax(avg_pred, axis=1)\n","y_pred = to_categorical(labels_pred)\n","score = accuracy_score(y_pred, y_test)\n","top_k_score = top_k_accuracy_score(avg_pred, y_test, k)\n","print('Test accuracy: {:.1f}%'.format(score * 100))\n","print('Top-{} accuracy: {:.1f}%'.format(k, top_k_score * 100))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Test accuracy: 99.2%\n","Top-5 accuracy: 100.0%\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Oe7uqzPhuKyB","colab_type":"code","colab":{},"outputId":"af8dbb6f-0dea-4535-da98-0ab7ddd972d1"},"source":["for i, model in enumerate(models):\n","    score = model.evaluate(x_test, y_test, verbose=0)\n","    print('Model {} test accuracy: {:.1f}%'.format(i + 1, score[1] * 100))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Model 1 test accuracy: 97.9%\n","Model 2 test accuracy: 98.6%\n","Model 3 test accuracy: 97.9%\n","Model 4 test accuracy: 98.3%\n","Model 5 test accuracy: 97.9%\n"],"name":"stdout"}]}]}